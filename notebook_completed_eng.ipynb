{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melusine + Arta's Workshop\n",
    "\n",
    "## Business Context &#128221;\n",
    "\n",
    "Your are working for an insurance company.\n",
    "\n",
    "Customers are contacting your company for two main reasons:\n",
    "\n",
    "* **Subscriptions:** subscribe, modify or close insurance policies.\n",
    "* **Claims:** initiate or follow the claim process of a covered loss (car, housing, etc).\n",
    "\n",
    "Many employees work on processing client emails through a custom user interface (i.e., a business application).\n",
    "\n",
    "## Data Science Context &#x1F575;\n",
    "\n",
    "You are a Data Scientist in charge of optimizing the email pre-processing (i.e., before being processed by a human).\n",
    "\n",
    "For that purpose your goal is to automatize:\n",
    "\n",
    "* The definition of the *priority* of an email to: low, normal or high.\n",
    "* The categorization of an email into two target business units: claims and subscriptions.\n",
    "* The reply in case of a thank you email.\n",
    "\n",
    "Two open-source packages must be used:\n",
    "\n",
    "* [Melusine](https://github.com/MAIF/melusine): Framework for automatic email processing.\n",
    "\n",
    "> Use: create a detection pipeline and use it to identify relevant patterns in the emails.\n",
    "\n",
    "* [Arta](https://github.com/MAIF/arta): A Python Rules Engine\n",
    "\n",
    "> Use: apply rules to translate the detection outputs into clear actions or extra data needed by the business application.\n",
    "\n",
    "\n",
    "First, let's make sure those packages are installed in your *python virtual environment*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'melusine>=3.2' arta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "During this workshop, you will use a demo dataset containing a variety of emails in French and in English.\n",
    "In the first part, you can explore the dataset, then you will use **Melusine** and **Arta** to qualify (i.e., pre-process) the emails.\n",
    "\n",
    "\n",
    "Let's import the dataset and take a sneak peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"workshop_email_data.json\")\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melusine: a processing pipeline\n",
    "\n",
    "### Email Preprocessing\n",
    "\n",
    "**Melusine** is a python framework to automatize emails processing.\n",
    "\n",
    "One of the key feature is the default *preprocessing pipeline* which does the following:\n",
    "\n",
    "1. **Cleaning**: remove undesired characters, replace non-breaking spaces, handle specific combinations (ex: \"œ\" => \"oe\").\n",
    "1. **Email Segmentation**: separate an email conversation with mutiple replies and transfers into a list of individual messages.\n",
    "1. **Message Tagging**: tag the different parts of the message (typically to get rid of unwanted footers and signature text).\n",
    "\n",
    "These steps can be edited through configuration files or class inheritance but for the sake of this workshop we will stick with the default preprocessing pipeline.\n",
    "\n",
    "Let's import and run this pipeline on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from melusine.pipeline import MelusinePipeline\n",
    "\n",
    "\n",
    "pipe = MelusinePipeline.from_config(\"preprocessing_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = pipe.transform(df)\n",
    "\n",
    "# Look at last columns\n",
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  &#x1F449; Your turn &#x1F477;\n",
    "\n",
    "Inspect the message corpus:\n",
    "\n",
    "* Print the first and second messages of the dataset.\n",
    "* Print all messages and their associated tags.\n",
    "* Extract the part tagged as \"BODY\" of any message of your choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st message\n",
    "print(preprocessed_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd message's body\n",
    "print(preprocessed_df.iloc[1][\"det_clean_last_body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and print the last message of the conversation (check the tags at the left)\n",
    "first_mail = preprocessed_df.iloc[1]\n",
    "last_msg = first_mail[\"messages\"][0]\n",
    "print(last_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print its tags\n",
    "last_msg.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the BODY tag\n",
    "body_tags = [elem for elem in last_msg.tags if elem[\"refined_tag\"] == \"BODY\" ]\n",
    "for tag_data in body_tags:\n",
    "    print(f\"Tag: {tag_data['refined_tag']} ==text==> {tag_data['base_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MelusineRegex\n",
    "\n",
    "The `MelusineRegex` is a convenient class to simplify detections using **regexes**. Main features are:\n",
    "\n",
    "* Name \"regex patterns\" for readable and explainable results.\n",
    "* Easily define patterns to be ignored and patterns that prevent matching (forget complex and unreadable negative look behind mecanisms).\n",
    "* Keep matching and non-matching exemples close to the regex definition to avoid loosing track of what the regex does.\n",
    "\n",
    "We will provide a working regex for this workshop: `EmergencyRegex`. This one will be used later to detect urgent requests in the emails.\n",
    "\n",
    "*N.B: You can check the code of the class [MelusineRegex](https://github.com/MAIF/melusine/blob/29815b578ba852b7c79116a926ef0a0e3bd0e1d5/melusine/base.py#L314), in particular docstrings and methods' type hints.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from melusine.base import MelusineRegex\n",
    "\n",
    "\n",
    "class EmergencyRegex(MelusineRegex):\n",
    "    \n",
    "    @property\n",
    "    def positive(self):\n",
    "        \"\"\"At least one of these patterns has to match for a global match of the MelusineRegex\"\"\"\n",
    "        return {\n",
    "            \"regex_1\": \"urgen(t|ce)|emergency\",\n",
    "            \"regex_2\": r\"as soon as possible\", \n",
    "            \"regex_3\": r\"(d[èe]s que|le plus vite|aussi vite que) possible\",\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def neutral(self):\n",
    "        \"\"\"Text matched by these patterns will be ignored (no influence on the global match of the MelusineRegex)\"\"\"\n",
    "        return None\n",
    "    \n",
    "    @property\n",
    "    def negative(self):\n",
    "        \"\"\"Any matching pattern will prevent the global match of the MelusineRegex\"\"\"\n",
    "        return r\"urgences\"\n",
    "    \n",
    "    @property\n",
    "    def match_list(self):\n",
    "        \"\"\"Text exemples that MUST be matched by the MelusineRegex\"\"\"\n",
    "        return [\n",
    "            \"C'est urgent\",\n",
    "            \"Ceci est une URGENCE\",\n",
    "            \"Call me as soon as possible\",\n",
    "        ]\n",
    "    \n",
    "    @property\n",
    "    def no_match_list(self):\n",
    "        \"\"\"Text exemples that MUST NOT be matched by the MelusineRegex\"\"\"\n",
    "        return [\n",
    "            \"Je travaille dans le service des urgences\",\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate your new MelusineRegex\n",
    "reg = EmergencyRegex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  &#x1F449; Your turn &#x1F477;\n",
    "\n",
    "Make sure that the `match_list` and `no_match_list` are coherent with the defined regexes (`positive()`).\n",
    "\n",
    "If it is coherent, nothing happens, if not, the test method will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MelusineDetector\n",
    "\n",
    "Business requests may become more and more complex over time.\n",
    "\n",
    "E.g., \"Could you handle this weird edge case when the email is sent from an autonomous vehicle?\"\n",
    "\n",
    "The objective of the `MelusineDetector` is to standardise how *detections* are performed on emails and restrain the technical debt that occurs during the life of an application.\n",
    "\n",
    "The key elements of a `MelusineDetector` are:\n",
    "\n",
    "1. **Declaration of input and output columns**: missing columns in the input DataFrame can be identified early-on => Keep track of which detector created a column.\n",
    "1. **pre_detect()**: processing to be done prior to detection. Typically assemble the text of interest.\n",
    "1. **detect()**: performed the core detection using regex, machine learning model or heuristics.\n",
    "1. **post_detect()**: combine the detection outputs to product the final detector result.\n",
    "\n",
    "You can find more details in the Melusine documentation on [MelusineDetector tutorials](https://maif.github.io/melusine/tutorials/05a_MelusineDetectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install unidecode, needed by the detector\n",
    "! pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from melusine.base import MelusineDetector\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "class EmergencyDetector(MelusineDetector):\n",
    "    \"\"\"A detector to detect emergency in emails\"\"\"\n",
    "\n",
    "    def __init__(self, name=\"emergency_detector\"):\n",
    "        self.regex = EmergencyRegex()\n",
    "\n",
    "    @property\n",
    "    def input_columns(self):\n",
    "        \"\"\"Input columns required by the detector\"\"\"\n",
    "        return [\"messages\", \"header\"]\n",
    "\n",
    "    @property\n",
    "    def output_columns(self):\n",
    "        \"\"\"Output columns created by the detector\"\"\"\n",
    "        return [\"emergency_detector_result\"]\n",
    "\n",
    "    def pre_detect(self, row, debug_mode=False):\n",
    "        \"\"\"Data transformations prior to the core detection\"\"\"\n",
    "        last_body = row[\"messages\"][0].extract_text(target_tags=[\"BODY\", \"THANKS\"])\n",
    "        row[\"effective_text\"] = row[\"header\"] + \"\\n\" + unidecode(last_body).lower()\n",
    "        return row\n",
    "\n",
    "    def detect(self, row, debug_mode=False):\n",
    "        \"\"\"Core detection method\"\"\"\n",
    "        match_data = self.regex(row[\"effective_text\"])\n",
    "        row[\"emergency_detector_result\"] = match_data[\"match_result\"]\n",
    "        \n",
    "        if debug_mode:\n",
    "            row[self.DEBUG_DICT_COL] = match_data\n",
    "            \n",
    "        return row\n",
    "    \n",
    "    def post_detect(self, row, debug_mode=False):\n",
    "        \"\"\"Data transformations posterior to the core detection\"\"\"\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emergency_detector = EmergencyDetector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  &#x1F449; Your turn &#x1F477;\n",
    "\n",
    "Run the detector on the email corpus and checkout the emergency detector's result (`emergency_detector_result` column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emergency = emergency_detector.transform(preprocessed_df)\n",
    "df_emergency[[\"det_clean_last_body\", \"emergency_detector_result\"]][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, implement a `ThanksRegex` and integrate it to a `ThanksDetector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ThanksRegex(MelusineRegex):\n",
    "\n",
    "    @property\n",
    "    def positive(self):\n",
    "        return {\n",
    "            \"thanks_fr\": r\"\\b(re)?mercie?\\b\",\n",
    "            \"thanks_en\": r\"\\bth(an|na)ks?( you)?\\b\",\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def neutral(self):\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def negative(self):\n",
    "        return {\n",
    "            \"negative_thanks_fr\": r\"merci de\",\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def match_list(self):\n",
    "        return [\n",
    "            \"Merci beaucoup\",\n",
    "            \"Je vous remercie\",\n",
    "            \"THANK YOU SO MUCH !\",\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def no_match_list(self):\n",
    "        return [\n",
    "            \"Merci de m'envoyer les document rapidement\",\n",
    "        ]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ThanksDetector(MelusineDetector):\n",
    "\n",
    "    def __init__(self, name=\"thanks_detector\"):\n",
    "        self.regex = ThanksRegex()\n",
    "\n",
    "    @property\n",
    "    def input_columns(self):\n",
    "        return [\"messages\", \"header\"]\n",
    "\n",
    "    @property\n",
    "    def output_columns(self):\n",
    "        return [\"thanks_detector_result\"]\n",
    "\n",
    "    def pre_detect(self, row, debug_mode=False):\n",
    "        last_body = row[\"messages\"][0].extract_text(target_tags=[\"BODY\", \"THANKS\"])\n",
    "        row[\"effective_text\"] = row[\"header\"] + \"\\n\" + unidecode(last_body).lower()\n",
    "        return row\n",
    "\n",
    "    def detect(self, row, debug_mode=False):\n",
    "        match_data = self.regex(row[\"tmp_clean_body\"])\n",
    "        row[\"thanks_detector_result\"] = match_data[\"match_result\"]\n",
    "\n",
    "        if debug_mode:\n",
    "            row[self.DEBUG_DICT_COL] = match_data\n",
    "\n",
    "        return row\n",
    "\n",
    "    def post_detect(self, row, debug_mode=False):\n",
    "        return row"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "thanks_detector = ThanksDetector()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, implement a `RoutingRegex` and integrate it to a `RoutingDetector`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RoutingRegex(MelusineRegex):\n",
    "\n",
    "    @property\n",
    "    def positive(self):\n",
    "        return {\n",
    "            \"contrat\": r\"\\b(contrat|resil|souscri|attestation)\",\n",
    "            \"sinistre\": r\"\\b(sinistre|accident|facture|repar[ea]|claim)\",\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def neutral(self):\n",
    "        ...\n",
    "\n",
    "    @property\n",
    "    def negative(self):\n",
    "        ...\n",
    "\n",
    "    @property\n",
    "    def match_list(self):\n",
    "        return [\n",
    "            \"Je souhaite souscrire à un contrat d'assurance\",\n",
    "            \"Suite à mon sinistre du 10/02/2023\",\n",
    "            \"Ma voiture est en réparation\",\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def no_match_list(self):\n",
    "        return [\n",
    "            \"Vous avez gagné 1000000€, cliquez ici pour les recevoir.\",\n",
    "        ]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RoutingDetector(MelusineDetector):\n",
    "\n",
    "    def __init__(self, name=\"routing_detector\"):\n",
    "        self.regex = RoutingRegex()\n",
    "\n",
    "    @property\n",
    "    def input_columns(self):\n",
    "        return [\"body\"]\n",
    "\n",
    "    @property\n",
    "    def output_columns(self):\n",
    "        return [\"routing_detector_result\"]\n",
    "\n",
    "    def pre_detect(self, row, debug_mode=False):\n",
    "        row[\"tmp_clean_body\"] = unidecode(row[\"body\"]).lower()\n",
    "        return row\n",
    "\n",
    "    def detect(self, row, debug_mode=False):\n",
    "        match_data = self.regex(row[\"tmp_clean_body\"])\n",
    "        row[\"routing_detector_match_data\"] = match_data\n",
    "\n",
    "        if debug_mode:\n",
    "            row[self.DEBUG_DICT_COL] = match_data\n",
    "\n",
    "        return row\n",
    "\n",
    "    def post_detect(self, row, debug_mode=False):\n",
    "        positive_match_data = row[\"routing_detector_match_data\"][\"positive_match_data\"]\n",
    "        if (\"sinistre\" in positive_match_data) and (\"contrat\" in positive_match_data):\n",
    "            routing = \"Autre\"\n",
    "        elif \"sinistre\" in positive_match_data:\n",
    "            routing = \"Sinistre\"\n",
    "        elif \"contrat\" in positive_match_data:\n",
    "            routing = \"Contrat\"\n",
    "        else:\n",
    "            routing = \"Autre\"\n",
    "\n",
    "        row[\"routing_detector_result\"] = routing\n",
    "\n",
    "        return row"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "routing_detector = RoutingDetector()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MelusinePipeline\n",
    "\n",
    "The `MelusinePipeline` object (inheriting from the sklearn Pipeline object) lets you assemble all the detectors into a pipeline that can be executed with a simple `transform()` method. It also brings features like:\n",
    "   * input columns check (raises an error if mandatory input columns are missing)\n",
    "   * pipeline load from (YAML) configuration files.\n",
    "\n",
    "Let's assemble the pipeline with the detectors we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from melusine.pipeline import MelusinePipeline\n",
    "\n",
    "pipe = MelusinePipeline([\n",
    "    (\"emergency_detector\", emergency_detector),\n",
    "    (\"thanks_detector\", thanks_detector),\n",
    "    (\"routing_detector\", routing_detector),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, let's run the pipeline on our DataFrame:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_transformed = pipe.transform(preprocessed_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arta: a rules engine\n",
    "\n",
    "### Use Case\n",
    "\n",
    "Thanks to the previous **Melusine** pipeline, we are able to process emails and get the following extra details:\n",
    "\n",
    "* Is it an urgent email?\n",
    "* Is it about claims or subscriptions?\n",
    "* Is it a thank you email?\n",
    "\n",
    "But this new data is not enough. Remember, we need to define for each email a *priority* and if we need an *automatic reply* or not.\n",
    "\n",
    "After talking with the people in charge of handling the customers emails, we defined the following business rules:\n",
    "\n",
    "#### Email Priority\n",
    "\n",
    "1. If an email is urgent, regardless of the business unit (claims or subscriptions), the priority should be set to \"high\".\n",
    "2. If an email is a thank you email for the claims, priority should be set to \"low\".\n",
    "3. Other cases should be set to \"medium\".\n",
    "\n",
    "#### Automatic Reply\n",
    "\n",
    "1. If the email is about subscriptions and is a thank you email, you should send the automatic reply which id is \"THANK_U_4_YOUR_THANK_U\".\n",
    "2. Do nothing for other cases.\n",
    "\n",
    "\n",
    "**Arta**`s goal is to simplify *rules definition* and *rules execution** in order to produce the expected outputs (priority and automatic reply here).\n",
    "\n",
    "To do that, we simply need to:\n",
    "\n",
    "1. Prepare the input data.\n",
    "1. Define the business rules.\n",
    "1. Implement the needed functions.\n",
    "1. Use the rules engine to apply the rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data\n",
    "\n",
    "In **Arta**, `input_data` is the data fed to the rules engine. In other words, it is the data on which the rules are applied.\n",
    "\n",
    "It must be a [mapping](https://docs.python.org/3/glossary.html#term-mapping) object (i.e., a dictionary like object).\n",
    "\n",
    "This dictionary could be an example:\n",
    "\n",
    "```python\n",
    "input_data = {\n",
    "    \"id\": 42,\n",
    "    \"cleaned_body\": \"Hi, I tried to reach you but unfortunately blah blah...\", \n",
    "    \"business\": \"CLAIMS\",\n",
    "    \"emergency\": True, \n",
    "    \"thank_you\": False,\n",
    "}\n",
    "```\n",
    "\n",
    "First, we are going to develop a function that maps the Melusine pipeline's output to the Arta's input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  &#x1F449; Your turn &#x1F477;\n",
    "\n",
    "Implement a function to convert a line (i.e., a `pandas.Series`) of the `DataFrame` referenced in the `processed_df` variable into a python dictionary with the same schema as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dict(row):\n",
    "    line = row.to_dict()\n",
    "    result = {}\n",
    "    result[\"id\"] = row.name\n",
    "    result[\"cleaned_body\"] = line[\"det_clean_last_body\"]\n",
    "    # result[\"business\"] = \"CLAIMS\" if line[\"claims_detector_result\"] else \"SUBSCRIPTIONS\"\n",
    "    result[\"emergency\"] = line[\"emergency_detector_result\"]\n",
    "    # result[\"thank_you\"] = line[\"thank_you_detector_result\"]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "input_data = convert_to_dict(df_transformed.iloc[0])\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule Group\n",
    "\n",
    "In **Arta**, a **rule group** is grouping different **rules** with a common output (generally, but not always).\n",
    "\n",
    "In our case, we could define 2 groups:\n",
    "\n",
    "1. `priority`\n",
    "1. `auto_reply`\n",
    "\n",
    "**Rule groups** are made of **rules** and they are bundled into a **rule set**. \n",
    "All of these are defined in a configuration file (i.e., *YAML file*) as you can see in the [documentation](https://maif.github.io/arta/how_to/#rule-set-and-rule-group)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  &#x1F449; Your turn &#x1F477;\n",
    "\n",
    "Define above rules using the following template. You can name the **validation functions* as you want, you will implement them later.\n",
    "\n",
    "*Hint:  use the [standard conditions](https://maif.github.io/arta/how_to/#simple-condition).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Template:\n",
    "\n",
    "```yaml\n",
    "---\n",
    "rules:\n",
    "  default_rule_set:\n",
    "    admission:\n",
    "# Complete below\n",
    "        \n",
    "\n",
    "actions_source_modules:\n",
    "  - assets.actions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "\n",
    "```yaml\n",
    "---\n",
    "rules:\n",
    "  default_rule_set:\n",
    "    priority:\n",
    "      HIGH:\n",
    "        simple_condition: input.emergency==True\n",
    "        action: set_priority\n",
    "        action_parameters:\n",
    "          value: HIGH\n",
    "      LOW:\n",
    "        simple_condition: input.thank_you==True and input.business==\"CLAIMS\"\n",
    "        action: set_priority\n",
    "        action_parameters:\n",
    "          value: LOW     \n",
    "      MEDIUM:\n",
    "        simple_condition: null\n",
    "        action: set_priority\n",
    "        action_parameters:\n",
    "          value: MEDIUM\n",
    "    auto_reply:\n",
    "      THANK_YOU:\n",
    "        simple_condition: input.thank_you==True and input.business==\"SUBSCRIPTIONS\"\n",
    "        action: send_reply\n",
    "        action_parameters:\n",
    "          reply_id: THANK_U_4_YOUR_THANK_U\n",
    "\n",
    "actions_source_modules:\n",
    "  - assets.actions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When your are done, copy and paste these rules into a new YAML file: `./assets/rules.yaml` (file name of your choice)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Functions\n",
    "\n",
    "A *rule* is made of *conditions* and one *action*. When conditions are validated (i.e., True) the *action* is executed.\n",
    "\n",
    "In fact, *actions* are just regular python *function* and they are named **action functions**.\n",
    "\n",
    "If you go back to the YAML definition of your rules, you can see:\n",
    "\n",
    "```yaml\n",
    "        action: set_priority\n",
    "        action_parameters:\n",
    "          value: MEDIUM\n",
    "```\n",
    "\n",
    "* `action:` is the name of your action function.\n",
    "* `action_parameters:` are the names and the values of the arguments passed to the function when executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  &#x1F449; Your turn &#x1F477;\n",
    "\n",
    "Look at your rules and list the validation functions that you need to implement:\n",
    "\n",
    "* ...\n",
    "* ...\n",
    "\n",
    "Now, implement them in the following python module `rules/actions.py`. \n",
    "\n",
    "*Hint: look at the following [documentation page](https://maif.github.io/arta/a_simple_example/#actions).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_priority(value, **kwargs):\n",
    "    return {\"priority\": value}\n",
    "\n",
    "\n",
    "def send_reply(reply_id, **kwargs):\n",
    "    # Fake rest api call \n",
    "    print(f\"The following email model was automatically sent: {reply_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule Engine\n",
    "\n",
    "Last but not least, we need to apply the rules on our data, it is the purpose of the rules engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  &#x1F449; Your turn &#x1F477;\n",
    "\n",
    "Instanciate a rules engine and apply the rules on the **Melusine** pipeline results for one email.\n",
    "\n",
    "*Hint: [Usage documentation](https://maif.github.io/arta/how_to/#usage).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rules engine instanciation\n",
    "from arta import RulesEngine\n",
    "\n",
    "\n",
    "eng = RulesEngine(config_path=\"./assets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply rules on one email\n",
    "input_data = convert_to_dict(df_transformed.iloc[0])\n",
    "\n",
    "results = eng.apply_rules(input_data)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  &#x1F449; Your turn &#x1F477;\n",
    "\n",
    "Apply on all emails and print their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply rules\n",
    "def apply_rules(row, rules_engine):\n",
    "    input_data = convert_to_dict(row)\n",
    "    result = rules_engine.apply_rules(input_data)\n",
    "    return {\n",
    "        \"priority\": result[\"priority\"][\"priority\"],\n",
    "        \"auto_reply\": result[\"auto_reply\"],\n",
    "    }\n",
    "\n",
    "# Run on all emails\n",
    "df_transformed[[\"priority\", \"auto_reply\"]] = df_transformed.apply(apply_rules, axis=1, rules_engine=eng, result_type='expand')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compare the obtained results with expected ones."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_delta = df_transformed[df_transformed[\"priority\"] != df_transformed[\"expected_priority\"]]\n",
    "i = 0\n",
    "print(f\"Obtained priority: {df_delta['priority'][i]['priority']}\")\n",
    "print(f\"Expected priority: {df_delta['priority'][i]['expected_priority']}\")\n",
    "print(df_delta['priority'][i])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End\n",
    "\n",
    "Now we can feed the employee business application with the emails and the needed extra informations. Automatic replies and prioritization will speed up the email processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
